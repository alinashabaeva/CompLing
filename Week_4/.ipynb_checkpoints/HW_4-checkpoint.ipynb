{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt').read()\n",
    "news = open('lenta.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fbe5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07a22cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ce8f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "853a2b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 55892),\n",
       " ('в', 48853),\n",
       " ('не', 46602),\n",
       " ('на', 29660),\n",
       " ('что', 26668),\n",
       " ('я', 21734),\n",
       " ('а', 21310),\n",
       " ('с', 21080),\n",
       " ('это', 17727),\n",
       " ('ты', 15469)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "044b0878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 72412),\n",
       " ('и', 33290),\n",
       " ('на', 28434),\n",
       " ('по', 19490),\n",
       " ('что', 17031),\n",
       " ('с', 15921),\n",
       " ('не', 12702),\n",
       " ('из', 7727),\n",
       " ('о', 7515),\n",
       " ('как', 7514)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd70ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.030066580918921042),\n",
       " ('в', 0.02628001641794979),\n",
       " ('не', 0.02506911192985684),\n",
       " ('на', 0.015955320798239428),\n",
       " ('что', 0.014345802260534357),\n",
       " ('я', 0.011691602907246653),\n",
       " ('а', 0.011463516055646737),\n",
       " ('с', 0.011339789697467536),\n",
       " ('это', 0.009536074571489897),\n",
       " ('ты', 0.008321404498582796)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "564123bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04808907489694771),\n",
       " ('и', 0.0221080111489724),\n",
       " ('на', 0.018883123731146926),\n",
       " ('по', 0.012943380513471676),\n",
       " ('что', 0.011310349590812525),\n",
       " ('с', 0.01057319451795703),\n",
       " ('не', 0.008435444806676101),\n",
       " ('из', 0.005131529052211166),\n",
       " ('о', 0.00499073907433246),\n",
       " ('как', 0.0049900749706632205)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3df9006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f563f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dvach = sentences_dvach [-50:]\n",
    "test_news = sentences_news[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a42aff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = sentences_dvach[:-50]\n",
    "sentences_news = sentences_news[:-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "425b0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "015aa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "012ab38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> а', 7826),\n",
       " ('<start> <start> я', 5949),\n",
       " ('<start> <start> и', 4120),\n",
       " ('<start> <start> в', 4047),\n",
       " ('<start> <start> ну', 3927),\n",
       " ('<start> <start> но', 3273),\n",
       " ('<start> <start> это', 3041),\n",
       " ('<start> <start> не', 2968),\n",
       " ('<start> <start> ты', 2925),\n",
       " ('<start> <start> да', 2596)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75ad2668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> в', 7961),\n",
       " ('<start> <start> по', 6210),\n",
       " ('<start> <start> как', 3736),\n",
       " ('<start> <start> однако', 1693),\n",
       " ('<start> <start> на', 1642),\n",
       " ('<start> <start> об', 1618),\n",
       " ('<start> об этом', 1578),\n",
       " ('<start> <start> он', 1551),\n",
       " ('<start> по словам', 1549),\n",
       " ('сообщает риа новости', 1324)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c3292ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "id2unigram_dvach = list(unigrams_dvach)\n",
    "word2id_dvach_uni = {word:i for i, word in enumerate(id2unigram_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {word:i for i, word in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach_uni[word3]] =  (trigrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c1a5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = csr_matrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e90b68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "id2unigram_news = list(unigrams_news)\n",
    "word2id_news_uni = {word:i for i, word in enumerate(id2unigram_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news_uni[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d331de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3c0a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2unigram, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2unigram[chosen])\n",
    "    \n",
    "        if id2unigram[chosen] == '<end>':\n",
    "            chosen = bigram2id['<start> <start>']\n",
    "        else:\n",
    "            bigram = id2bigram[current_idx]\n",
    "            trigram = bigram + ' ' + id2unigram[chosen]\n",
    "            w1, w2, w3 = trigram.split()\n",
    "            new_bigram = w2 + ' ' + w3\n",
    "            chosen = bigram2id[new_bigram]\n",
    "            \n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4b921c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ну хуй тя знает вдруг ты ей не будет \n",
      "\n",
      " согласно твоим словам ты не можешь понять \n",
      "\n",
      " но густав объяснил ей что идет от эрогенных зон слуховых зон и т д и так по среднему нету возможности \n",
      "\n",
      " те кто сидят и конечно десяток раскаченных геев-порноактеров плюс пяток помельче дрищей-карланов и вот к примеру на том самом озере который в отличии от некоторых \n",
      "\n",
      " пруфы будут если выявят значительный расход эл \n",
      "\n",
      " я ещё раз доказывает мощь лигивона умей в аналогию \n",
      "\n",
      " не небольшая доля пробивается огромные таланты просто очень эмоциональные мозгом совсем не обязательно \n",
      "\n",
      " его скорее всего\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2unigram_dvach, bigram2id_dvach).replace('<end>', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53f1c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "как видно провокации не прекратились даже при наличии неоспоримых доказательств подделки по меньшей мере 5 миллионов тонн фуражного зерна таком же фугасе два дня после того как вчера они упали на 18 копеек до отметки 5.237 пунктов \n",
      "\n",
      " далее встреча продолжилась за закрытыми дверями в связи с начатым расследованием выдвинутых против него 29 августа а второй получил тяжелое ранение в голову чтобы заманить к себе побольше пользователей \n",
      "\n",
      " в 5 00 на территории ингушетии растет число заболевших острыми кишечными инфекциями превысило 1200 \n",
      "\n",
      " адвокатов к главе медиа-моста владимиру гусинскому и гендиректору нтв олегу добродееву российская нефтяная компания роснефть прекратила поставки\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2bigram_news, id2unigram_news, bigram2id_news).replace('<end>', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04fffe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd11f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "for sentence in test_news[:10]:\n",
    "    prob = []\n",
    "    \n",
    "    for ngram in ngrammer(sentence, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "\n",
    "        gram2 = word1 + ' ' + word2\n",
    "        if gram2 in bigrams_news and ngram in trigrams_news:\n",
    "            prob.append(np.log(trigrams_news[ngram] / bigrams_news[gram2]))\n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "    \n",
    "    perplexities.append(perplexity(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f45119cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1397.4872256326555,\n",
       " 12124.078943489689,\n",
       " 15607.318565312942,\n",
       " 87103.55213154675,\n",
       " 22156.341037630442,\n",
       " 15305.829596378455,\n",
       " 27139.45542712739,\n",
       " 13911.558568619903,\n",
       " 15601.855410620083,\n",
       " 8299.40561950905]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e834573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21864.688252586737"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1204671",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "for sentence in test_dvach[:10]:\n",
    "    prob = []\n",
    "    \n",
    "    for ngram in ngrammer(sentence, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "\n",
    "        gram2 = word1 + ' ' + word2\n",
    "        if gram2 in bigrams_dvach and ngram in trigrams_dvach:\n",
    "            prob.append(np.log(trigrams_dvach[ngram] / bigrams_dvach[gram2]))\n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "    \n",
    "    perplexities.append(perplexity(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "feb75ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70247.22942196298,\n",
       " 1453.312720712195,\n",
       " 77748.62515781814,\n",
       " 100000.00000000012,\n",
       " 46104.39983753712,\n",
       " 7650.66734746975,\n",
       " 18025.826375016793,\n",
       " 2746.3634966563855,\n",
       " 110.34264814658026,\n",
       " 8230.617749535151]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7951df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33231.738475485516"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600b01d",
   "metadata": {},
   "source": [
    "2 других способа определить вероятность несловарных слов: \n",
    "\n",
    "1)\tрешить эту проблему так же, как и с фиксированным словарем:\n",
    "a)\tвыбрать заранее заданный словарь (список слов);\n",
    "b)\tна этапе нормализации текста в обучающей выборке преобразовать любое слово, не входящее в эту выборку (т.е. несловарное слово), в токен несловарного слова (<UNK>);\n",
    "c)\tоценить вероятность <UNK> по тому, сколько раз оно встречается в тексте (как и вероятность любого другого обычного слова).\n",
    "\n",
    "2)\tЕсли такого заранее заданного словаря нет, то можно его создать (неявно), заменив некоторые слова в зависимости от их частотности на <UNK>. Например, можно заменить на <UNK> все слова, которые встречаются менее n раз, где n – это какое-то небольшое число. Или заранее выбрать размер словаря и выбрать топ слов по частотности, а остальные заменить на <UNK>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8ef69",
   "metadata": {},
   "source": [
    "Существуют слова, которые есть в словаре (то есть это не несловарные слова), но они появляются в тестовой выборке в неизвестном контексте (например, появляются после слова, после которого никогда не появлялись в обучающей выборке). Чтобы языковая модель не приписывала нулевую вероятность таким «невидимым» словам, надо немного снизить вероятность наиболее частотных слов и «отдать» ее таким словам. Это и есть сглаживание (smoothing). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
